\documentclass[12pt]{report}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{natbib}%referencing
\usepackage[left=1.0in,right=1.0in,top=.8in,bottom=.8in]{geometry}
\usepackage{float}
\linespread{1.3}
\usepackage{graphicx}%figures
\usepackage{rotating}%landscape
\usepackage{amsmath}%math
\usepackage{titlesec} %formatting chapters
\titlespacing*{\chapter}{-15pt}{10pt}{15pt}
\titlespacing*{\section}{0pt}{0pt}{5pt}
\titlespacing*{\subsection}{0pt}{5pt}{5pt}
\titleformat{\chapter}[hang] 
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter.}{1em}{}
\renewcommand{\chaptername}{}
\graphicspath{{Images/}}%image folder name
\usepackage{graphicx}  % in the preamble
\usepackage[table]{xcolor}


%Cover page contents
\title{
{\includegraphics[scale=.3]{logotu.jpg}}\\
\uppercase\large{
    {Tribhuvan University}\\
    {Institute of Engineering}\\
    {Pulchowk Campus}\\
    \vspace{.5cm}
    {A \\Project Proposal\\On\\\textbf{Hybrid Dual-Verification Framework for Federated Learning using Zero-Knowledge Proofs}}\\
    \vspace{.5cm}
    {\textbf{Submitted By:}\\Bindu Paudel (PUL078BCT032) \\ Krishant Timilsina (PUL078BCT045)}\\
    \vspace{.5cm}
    {\textbf{Submitted To:}\\ Department of Electronics \& Computer Engineering}\\
                }
    }
\date{July, 2025}
\begin{document}
\maketitle
\pagenumbering{roman}
\setcounter{page}{2}

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{\numberline{}Acknowledgments}

We express our sincere gratitude to our supervisor, Associate Professor Arun Kumar Timalsina, Ph.D., Department of Electronics and Computer Engineering, Pulchowk Campus, for his constant support and guidance. We also thank our faculty, friends, and family members who supported us throughout this project proposal.


\tableofcontents
\addcontentsline{toc}{chapter}{\numberline{}Contents}

\clearpage
\begingroup
\setlength{\parskip}{0pt}  % No extra spacing between paragraphs
\setlength{\parindent}{0pt}  % No paragraph indentation

\listoffigures
\addcontentsline{toc}{chapter}{\numberline{}List of Figures}

\vspace{1em}  % optional space between sections

\listoftables
\addcontentsline{toc}{chapter}{\numberline{}List of Tables}

\vspace{1em}

\chapter*{List of Abbreviations}
\addcontentsline{toc}{chapter}{\numberline{}List of Abbreviations}
% Your abbreviations go here:
\begin{tabular}{ll}
\textbf{FL} & Federated Learning \\
\textbf{ZKP} & Zero-Knowledge Proof \\
\textbf{zk-STARK} & Scalable Transparent ARguments of Knowledge \\
\textbf{zk-SNARK} & Succinct Non-interactive ARguments of Knowledge \\
\textbf{FedJSCM} & Federated Joint Server-Client Momentum \\
\textbf{SGD} & Stochastic Gradient Descent \\
\textbf{NN} & Neural Network \\
\end{tabular}

\endgroup
\clearpage



\pagenumbering{arabic}

\chapter{Introduction}

\section{Background}
Federated Learning (FL) is a decentralized machine learning paradigm where multiple clients collaboratively train a shared global model while keeping their local data private. Instead of transmitting sensitive data to a central server, clients perform local training and share only model updates. This approach has gained traction in domains such as healthcare, finance, and mobile applications where data privacy is critical. Despite these advantages, FL is not immune to security threats.

One major challenge in FL is ensuring the integrity of the model training process. Malicious clients may submit poisoned updates to mislead the global model, while an untrusted server may incorrectly aggregate updates to favor certain clients or to manipulate the learning outcome. Traditional methods to mitigate such attacks rely on trust assumptions or statistical anomaly detection, which can be inadequate in adversarial settings. 

To address this, Zero-Knowledge Proofs (ZKPs) offer a powerful cryptographic tool that enables one party (the prover) to convince another (the verifier) that a statement is true without revealing any additional information. In the context of FL, ZKPs can be used by clients to prove the correctness of their local training and by the server to prove the correctness of aggregation.

This project introduces a novel federated learning framework that incorporates dual verification using ZKPs from both clients and the server. It leverages a hybrid approach—zk-STARKs for client-side proofs and zk-SNARKs (Groth16) for server-side proofs—to balance transparency, scalability, and efficiency. Moreover, it employs a dynamic proof mechanism that adjusts the rigor of verification based on the stability of the global model to reduce computational overhead while maintaining security.

\section{Problem Statement}
Current federated learning systems generally assume partial trust in either the clients or the server. Most implementations include client-side verification using ZKPs or anomaly detection but ignore the need for validating the server’s aggregation process. Moreover, existing ZKP-based approaches suffer from static proof granularity, leading to inefficiency in later, more stable training rounds. These limitations expose the system to model poisoning, aggregation tampering, and unnecessary performance bottlenecks.

\section{Objectives}
The key objectives of this project are:
\begin{itemize}
    \item To design a dual-verifiable federated learning framework where both clients and the server provide ZKPs.
    \item To implement zk-STARK-based proofs for verifying client-side local training and zk-SNARK (Groth16) proofs for verifying server-side aggregation.
    \item To introduce a dynamic proof adjustment mechanism that modifies proof rigor based on training stability.
    \item To ensure the framework is efficient and deployable on resource-constrained devices.
    \item To validate the system using real-world non-IID datasets and demonstrate robustness against adversarial attacks.
\end{itemize}

\section{Scope}
The proposed system aims to enhance the security and transparency of federated learning frameworks by removing the need to trust any single entity. The framework will be tested on privacy-sensitive and heterogeneous datasets like Medical MNIST and the Human Activity Recognition (HAR) dataset. Evaluation will be done in terms of proof generation time, verification latency, and training accuracy. The verification layer will be implemented using a smart contract on Ethereum or a private blockchain, ensuring public verifiability of the computation.

By combining momentum-based aggregation (FedJSCM) with zero-knowledge verification, this project also pioneers the development of a verifiable optimization algorithm in the FL domain, which is particularly significant for high-stakes applications such as medical diagnostics and financial forecasting.

\chapter{Literature Review}

\section{Related Work}
Federated Learning (FL) was popularized by Google \cite{mcmahan2017communication} as a method to collaboratively train models across decentralized devices holding sensitive data. Since then, multiple enhancements have been proposed to tackle communication efficiency, robustness to non-IID data, and privacy threats.

One branch of work focuses on making FL resilient to poisoning and Byzantine attacks. Techniques such as Krum \cite{blanchard2017machine}, Trimmed Mean, and Multi-Krum attempt to detect and filter out malicious updates based on statistical properties. However, these methods can fail under coordinated attacks and provide no formal guarantees.

In contrast, cryptographic methods offer stronger guarantees. Differential privacy \cite{geyer2017differentially} and secure aggregation \cite{bonawitz2017practical} are popular for privacy protection, but they do not verify the correctness of computations. Zero-Knowledge Proofs (ZKPs), on the other hand, allow verification without disclosing data or computation details. Early attempts like ZKFL \cite{zhu2021zkfl} introduced client-side zk-SNARK proofs for verifying SGD steps. However, they incur high computational cost and neglect the server's role. 

Our proposal builds upon these efforts by introducing a dual-verifiable FL framework. Clients use zk-STARKs for transparent, scalable proofs of correct local training, while the server uses Groth16 zk-SNARKs to prove correct aggregation. This hybrid approach balances performance and trust.

\section{Related Theory}
This section introduces the theoretical foundations of the proposed framework. We explain Federated Learning, Stochastic Gradient Descent, Zero-Knowledge Proofs, zk-STARKs, zk-SNARKs, and the FedJSCM algorithm.

\subsection{Federated Learning (FL)}
FL aims to minimize a global loss function \( L(w) \) defined over the data of all \( N \) clients:
\[
L(w) = \sum_{i=1}^N p_i L_i(w),
\]
where \( L_i(w) \) is the local loss function of client \( i \), and \( p_i \) is the relative data proportion (e.g., \( p_i = \frac{n_i}{\sum_j n_j} \)).

Each round, clients perform local updates using Stochastic Gradient Descent (SGD):
\[
w_i^{(t+1)} = w^{(t)} - \eta \nabla L_i(w^{(t)}),
\]
and send their model update (\( \Delta_i = w_i^{(t+1)} - w^{(t)} \)) to the server.

\subsection{Stochastic Gradient Descent (SGD)}
SGD is used to minimize a loss function \( L(w) \) by iteratively updating parameters using:
\[
w \leftarrow w - \eta \nabla L(w; x_i, y_i),
\]
where \( (x_i, y_i) \) is a mini-batch sample, and \( \eta \) is the learning rate. A valid SGD step requires computing gradients from actual data, which is what the client zk-STARKs prove.

\subsection{Zero-Knowledge Proofs (ZKPs)}
A ZKP allows a prover to convince a verifier that a computation was done correctly without revealing the inputs. Formally, a ZKP must satisfy:
\begin{itemize}
  \item \textbf{Completeness}: If the statement is true, an honest verifier is convinced.
  \item \textbf{Soundness}: If the statement is false, a cheating prover cannot convince the verifier.
  \item \textbf{Zero-Knowledge}: No information about the inputs is leaked.
\end{itemize}

\subsection{zk-STARKs}
zk-STARKs (Scalable Transparent ARguments of Knowledge) offer post-quantum security and transparency (no trusted setup). They operate over arithmetic intermediate representations (AIR), expressing computation as transition constraints over state variables.

For example, to prove correct SGD updates over \( k \) steps, we construct a trace \( T = [w_0, w_1, \dots, w_k] \) such that:
\[
\forall j,\ w_{j+1} = w_j - \eta \nabla L(w_j; x_j, y_j).
\]

These are encoded in a trace table and verified using low-degree polynomial tests (Reed-Solomon encoding) and Merkle trees. Although proofs are larger (hundreds of KB), they are fast to generate and verify.

\subsection{zk-SNARKs (Groth16)}
zk-SNARKs provide succinct and efficient proofs but require a trusted setup. Groth16 proves statements of the form:
\[
\text{Given: } x,\ \text{Prove: } \exists w : C(x, w) = 0,
\]
where \( C \) is an arithmetic circuit representing the computation. For example, the server can encode FedJSCM aggregation as:
\[
\text{new model } = \sum_{i=1}^N p_i \Delta_i + \beta m^{(t)}
\]
and prove that this was correctly computed without revealing individual \( \Delta_i \).

\subsection{FedJSCM Aggregation}
FedJSCM is a momentum-based aggregation technique that stabilizes FL under non-IID conditions. The momentum update rule is:
\[
m^{(t+1)} = \gamma m^{(t)} + \sum_{i=1}^N p_i \Delta_i,
\]
\[
w^{(t+1)} = w^{(t)} + m^{(t+1)},
\]
where \( \gamma \) is the momentum coefficient. This formulation accelerates convergence and avoids oscillations common in non-IID FL setups. Proving this with Groth16 ensures no tampering from the server.

\subsection{Dynamic Proof Granularity}
To balance security and efficiency, proof rigor is dynamically adjusted based on model stability. Metrics such as gradient norm or validation loss change guide the switch between full, partial, or lightweight proofs. For example:
\begin{itemize}
    \item \textbf{Unstable phase}: Full SGD trace proofs, per-round server proofs.
    \item \textbf{Stable phase}: One-step delta proof, server proof every 5 rounds.
\end{itemize}

This adaptive scheme reduces overhead without compromising verification guarantees.

\subsection{Blockchain-Based Verification}
ZKPs are submitted to a verification layer implemented using smart contracts on Ethereum or a private blockchain. The smart contract logic ensures that only valid updates are accepted, providing tamper-evidence and decentralized enforcement of computation integrity.




\chapter{Proposed Methodology}

The proposed methodology outlines a secure and efficient federated learning system that utilizes dual Zero-Knowledge Proofs (ZKPs) to ensure end-to-end verifiability. The methodology follows an iterative, round-based training structure, integrating cryptographic proof systems, adaptive rigor tuning, and blockchain-based verification.

\section{Overview}
Each training round in the federated system consists of three major phases:
\begin{enumerate}
    \item \textbf{Client-side training and proof generation} using zk-STARKs.
    \item \textbf{Server-side verification, aggregation, and proof generation} using Groth16 zk-SNARKs.
    \item \textbf{Blockchain verification layer} for decentralized validation of proofs.
\end{enumerate}
An additional control mechanism dynamically adjusts the granularity of proofs based on model stability metrics.

\section{Step-by-Step Procedure}

\subsection*{Step 1: Initialization}
\begin{itemize}
    \item The server initializes the global model \( w^{(0)} \), server momentum \( m^{(0)} = 0 \), and proof rigor parameters.
    \item The server distributes the initial model to all participating clients.
    \item Clients load their local data and prepare for training.
\end{itemize}

\subsection*{Step 2: Client-Side Operations}
For each round \( t \), each client \( i \) performs the following:
\begin{enumerate}
    \item Downloads global model \( w^{(t)} \).
    \item Computes model update \( \Delta_i^{(t)} \) by applying SGD:
    \[
    w_i^{(t+1)} = w^{(t)} - \eta \nabla L_i(w^{(t)}), \quad \Delta_i^{(t)} = w_i^{(t+1)} - w^{(t)}.
    \]
    \item Quantizes the update \( \Delta_i^{(t)} \) to 8-bit fixed point for efficient circuit representation.
    \item Generates a zk-STARK proof \( \pi_i^{\text{client}} \) for the statement:
    \begin{itemize}
        \item The model update \( \Delta_i^{(t)} \) was generated from SGD using valid, committed local data.
        \item The data used meets certain size and format requirements.
    \end{itemize}
    \item Sends \( (\Delta_i^{(t)}, \pi_i^{\text{client}}) \) to the server.
\end{enumerate}

\subsection*{Step 3: Server-Side Operations}
Upon receiving submissions from all clients:
\begin{enumerate}
    \item Verifies each \( \pi_i^{\text{client}} \) using batch zk-STARK verification.
    \item Filters out invalid updates.
    \item Applies FedJSCM aggregation:
    \[
    m^{(t+1)} = \gamma m^{(t)} + \sum_{i \in V} p_i \Delta_i^{(t)},
    \]
    \[
    w^{(t+1)} = w^{(t)} + m^{(t+1)}.
    \]
    where \( V \) is the set of verified clients and \( \gamma \) is the momentum coefficient.
    \item Generates a Groth16 zk-SNARK proof \( \pi^{\text{server}} \) proving that:
    \begin{itemize}
        \item Aggregation of updates and momentum calculation were done correctly.
        \item Public inputs include hashes of accepted \( \Delta_i^{(t)} \), \( w^{(t)} \), and \( w^{(t+1)} \).
    \end{itemize}
    \item Broadcasts \( w^{(t+1)} \) and \( \pi^{\text{server}} \) to clients and the blockchain verifier.
\end{enumerate}

\subsection*{Step 4: Blockchain-Based Verification}
\begin{itemize}
    \item A smart contract or consortium of verifier nodes checks:
    \begin{itemize}
        \item Validity of the server's zk-SNARK proof \( \pi^{\text{server}} \).
        \item Optionally, random sampling of client zk-STARK proofs \( \pi_i^{\text{client}} \).
    \end{itemize}
    \item If verification fails, the model is rejected and round \( t \) is invalidated.
    \item Otherwise, training continues to round \( t+1 \).
\end{itemize}

\subsection*{Step 5: Dynamic Proof Rigor Adjustment}
After each round, the server evaluates the following metrics:
\begin{itemize}
    \item Change in model accuracy on a held-out public or validation dataset.
    \item Magnitude of aggregated gradient updates (\( \| m^{(t)} \| \)).
    \item Time and resource cost of generating proofs.
\end{itemize}
Based on these, the server adjusts proof configurations:
\begin{itemize}
    \item \textbf{High Rigor}: Full SGD trace proofs (clients), every-round server proof.
    \item \textbf{Medium Rigor}: One-step update proof, server proof every 2 rounds.
    \item \textbf{Low Rigor}: Lightweight delta norm proof, server proof every 5 rounds.
\end{itemize}

\section{System Components and Tools}
\begin{itemize}
    \item \textbf{Clients}: Implemented using PySyft or Flower with Cairo-based zk-STARK circuits.
    \item \textbf{Server}: Runs aggregation and Circom-based zk-SNARK circuits using SnarkJS.
    \item \textbf{Blockchain}: Ethereum smart contract or private Quorum chain for verification logic.
    \item \textbf{Datasets}: Medical MNIST and HAR datasets (non-IID and privacy-sensitive).
    \item \textbf{Hardware}: Raspberry Pi for client simulation; AWS/GCP for server.
\end{itemize}

\section{Security and Efficiency Trade-offs}
\begin{itemize}
    \item zk-STARKs ensure scalability and transparency for clients.
    \item zk-SNARKs enable compact proofs suitable for on-chain verification.
    \item Quantized weights and dynamic proof control reduce computational overhead.
\end{itemize}

This methodology ensures verifiability, robustness, and efficiency across the entire FL pipeline, making it suitable for high-stakes and privacy-critical applications.


\chapter{Proposed Experimental Setup}

This chapter describes the experimental setup for evaluating the dual ZKP-based federated learning system, simulated on a cloud environment using multiple virtual machines (VMs) to replicate client-server interactions.

\section{Infrastructure Overview}

\subsection{Cloud Deployment}
We simulate a federated setup on AWS using the following:
\begin{itemize}
    \item \textbf{Server Node}: One VM as the central aggregator.
    \item \textbf{Client Nodes}: 5--10 VMs, each representing a federated client.
    \item \textbf{Blockchain Node}: One VM running a private Ethereum node for zk-SNARK verification.
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Role} & \textbf{Instance} & \textbf{Specs} \\
\hline
Server & t3.xlarge & 4 vCPUs, 16 GB RAM \\
Client & t3.medium & 2 vCPUs, 4 GB RAM \\
Blockchain & t3.small & 2 vCPUs, 2 GB RAM \\
\hline
\end{tabular}
\caption{AWS EC2 configuration}
\end{table}

\section{Software Stack}

\subsection{Client VMs}
\begin{itemize}
    \item OS: Ubuntu 22.04
    \item Framework: Flower with PyTorch and Cairo (for zk-STARKs)
\end{itemize}

\subsection{Server VM}
\begin{itemize}
    \item FL Server: Flower + FedJSCM
    \item zk-SNARK Prover: Circom + SnarkJS
\end{itemize}

\subsection{Blockchain Node}
\begin{itemize}
    \item Platform: Ethereum (private chain)
    \item Contract: Solidity verifier from SnarkJS
\end{itemize}

\section{Datasets}
\begin{itemize}
    \item MedMNIST (non-IID, split by class)
    \item UCI HAR (sensor time-series)
\end{itemize}
Each client holds \textasciitilde5--10\% of the dataset.

\section{Proof Configuration}
\begin{itemize}
    \item \textbf{Client (zk-STARK)}: Cairo circuits for SGD steps
    \item \textbf{Server (zk-SNARK)}: Groth16 aggregation proof in Circom
\end{itemize}


This setup enables reproducible and secure simulation of federated learning with privacy-preserving, verifiable computation.


\chapter{Proposed Experimental Setup (if any)}
\begin{itemize}
    \item \textbf{Datasets}: Medical MNIST, HAR dataset.
    \item \textbf{Client Devices}: Raspberry Pi (low-resource simulation).
    \item \textbf{ZKP Backends}: Cairo/StarkEx for clients, Circom + SnarkJS for server.
    \item \textbf{Blockchain}: Ethereum smart contracts for proof verification.
\end{itemize}

\chapter{System Design}

This chapter outlines the architectural design of the dual ZKP-based federated learning framework. It explains the data and proof flow, the interaction between system components, and the integration of verifiable computation via zk-STARKs and zk-SNARKs.

\section{Overview}
The system is divided into three primary domains:
\begin{itemize}
    \item \textbf{Client Layer (Edge Nodes)}: Simulates data owners who perform local model updates and generate zk-STARK proofs.
    \item \textbf{Server Layer (Aggregator)}: Aggregates models and verifies client proofs. It generates a zk-SNARK proof of correct aggregation.
    \item \textbf{Blockchain Layer}: A smart contract verifies the zk-SNARK proof on-chain for public verifiability.
\end{itemize}

\section{Architecture Diagram}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Images/arch.png}
    \caption{System Diagram}
    \label{fig:enter-label}
\end{figure}


\chapter{Timeline}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Phase} & \textbf{Aug} & \textbf{Sep} & \textbf{Oct} & \textbf{Nov} & \textbf{Dec} \\ \hline
Literature Review & \cellcolor{green!25} & & & & \\ \hline
Client Proof Setup & & \cellcolor{green!25} & & & \\ \hline
Server Aggregation & & & \cellcolor{green!25} & & \\ \hline
Blockchain Integration & & & & \cellcolor{green!25} & \\ \hline
Evaluation & & & & & \cellcolor{green!25} \\ \hline
\end{tabular}
\caption{Project Gantt chart}
\end{table}








% \begin{figure}[H]
%     \centering
%     \includegraphics[angle=90, width=0.35\linewidth]{Images/gantt.png}
%     \caption{Gantt Chart}
%     \label{fig:enter-label}
% \end{figure}


\addcontentsline{toc}{section}{References}
\renewcommand{\bibname}{References}
\bibliographystyle{unsrt}
\bibliography{ref}
\end{document}
